@inproceedings{yamada2020luke,
    title = "{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention",
    author = "Yamada, Ikuya  and
      Asai, Akari  and
      Shindo, Hiroyuki  and
      Takeda, Hideaki  and
      Matsumoto, Yuji",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.523",
    doi = "10.18653/v1/2020.emnlp-main.523",
    pages = "6442--6454"
}

@inproceedings{hvingelby2020dane,
    title = "{D}a{NE}: A Named Entity Resource for {D}anish",
    author = "Hvingelby, Rasmus  and
      Pauli, Amalie Brogaard  and
      Barrett, Maria  and
      Rosted, Christina  and
      Lidegaard, Lasse Malm  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.565",
    pages = "4597--4604",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{derc2014dkie,
    title = "{DKIE}: Open Source Information Extraction for {D}anish",
    author = "Derczynski, Leon  and
      Field, Camilla Vilhelmsen  and
      B{\o}gh, Kenneth S.",
    booktitle = "Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E14-2016",
    doi = "10.3115/v1/E14-2016",
    pages = "61--64",
}

@misc{derc2019simple,
      title={Simple Natural Language Processing Tools for Danish}, 
      author={Leon Derczynski},
      year={2019},
      eprint={1906.11608},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{johann2015udddt,
    author = {Anders Johannsen and Hector Martinez and Alonso Barbara Plank},
    title = {Universal Dependencies for {D}anish},
    year = {2015},
    booktitle = {TLT14},
}

@misc{christensen1998parole,
    author = {Ole Norling-Christensen and Britt-Katrin Keson and J{\o}rg Asmussen},
    title = {PAROLE-DK and ePAROLE: Morphosyntactically Annotated Danish Language Corpus},
    year = {1998},
    publisher = {The Society for Danish Language and Literature, DSL},
}
@inproceedings{plank2019neural,
    title = "Neural Cross-Lingual Transfer and Limited Annotated Data for Named Entity Recognition in {D}anish",
    author = "Plank, Barbara",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://www.aclweb.org/anthology/W19-6143",
    pages = "370--375",
    abstract = "Named Entity Recognition (NER) has greatly advanced by the introduction of deep neural architectures. However, the success of these methods depends on large amounts of training data. The scarcity of publicly-available human-labeled datasets has resulted in limited evaluation of existing NER systems, as is the case for Danish. This paper studies the effectiveness of cross-lingual transfer for Danish, evaluates its complementarity to limited gold data, and sheds light on performance of Danish NER.",
}

@inproceedings{pan2017wikiann,
    title = "Cross-lingual Name Tagging and Linking for 282 Languages",
    author = "Pan, Xiaoman  and
      Zhang, Boliang  and
      May, Jonathan  and
      Nothman, Joel  and
      Knight, Kevin  and
      Ji, Heng",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1178",
    doi = "10.18653/v1/P17-1178",
    pages = "1946--1958",
    abstract = "The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.",
}

@inproceedings{tjang2003conll,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://www.aclweb.org/anthology/W03-0419",
    pages = "142--147",
}


@inproceedings{rahimi2019transfer,
    title = "Massively Multilingual Transfer for {NER}",
    author = "Rahimi, Afshin  and
      Li, Yuan  and
      Cohn, Trevor",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1015",
    pages = "151--164",
}

@article{ramshaw1995IOB,
  author    = {Lance A. Ramshaw and
               Mitchell P. Marcus},
  title     = {Text Chunking using Transformation-Based Learning},
  journal   = {CoRR},
  volume    = {cmp-lg/9505040},
  year      = {1995},
  url       = {http://arxiv.org/abs/cmp-lg/9505040},
  timestamp = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/cmp-lg-9505040.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@inproceedings{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@online{botxo2019dabert,
  author = {BotXO},
  title = {BotXO has trained the most advanced Danish BERT model to date},
  year = 2019,
  url = {https://www.botxo.ai/en/blog/danish-bert-model/},
  urldate = {2021-02-28}
}

@misc{clark2020electra,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@phdthesis{bertelsen2020lctra,
  author  = {Malte H{\o}jmark-Bertelsen},
  title   = {Ælæctra - A Step Towards More Efficient Danish Natural Language Processing},
  school  = {Cognitive Science, Aarhus University},
  year    = {2020},
  type    = {Bachelor's Thesis}
}

@inproceedings{akbik2019flair,
  title={FLAIR: An easy-to-use framework for state-of-the-art NLP},
  author={Akbik, Alan and Bergmann, Tanja and Blythe, Duncan and Rasul, Kashif and Schweter, Stefan and Vollgraf, Roland},
  booktitle={{NAACL} 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages={54--59},
  year={2019}
}

@article{rfou2015polyglot,
        author = {Al-Rfou, Rami and Kulkarni, Vivek and Perozzi, Bryan and Skiena, Steven},
        title = {{Polyglot-NER}: Massive Multilingual Named Entity Recognition},
        journal = {{Proceedings of the 2015 {SIAM} International Conference on Data Mining, Vancouver, British Columbia, Canada, April 30 - May 2, 2015}},
        month     = {April},
        year      = {2015},
        publisher = {SIAM}
}

@InProceedings{manning2014corenlp,
  author    = {Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David},
  title     = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
  booktitle = {Association for Computational Linguistics (ACL) System Demonstrations},
  year      = {2014},
  pages     = {55--60},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-5010}
}

@INPROCEEDINGS{kromann2003ddt,
    author = {Matthias Trautner Kromann},
    title = {The Danish Dependency Treebank and the DTAG treebank tool},
    booktitle = {In Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT 2003},
    year = {2003},
    pages = {14--15}
}

@software{honnibal2020spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}

@article{vaswani2017att,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{hedderich2021survey,
    title={A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios}, 
    author={Michael A. Hedderich and Lukas Lange and Heike Adel and Jannik Strötgen and Dietrich Klakow},
    year={2021},
    eprint={2010.12309},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@book{russell2016ai,
  title={Artificial Intelligence: a modern approach},
  author={Russell, Stuart J. and Norvig, Peter},
  edition={3},
  year={2009},
  publisher={Pearson}
}

@article{otter18dlnlp,
  author    = {Daniel W. Otter and
               Julian R. Medina and
               Jugal K. Kalita},
  title     = {A Survey of the Usages of Deep Learning in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1807.10854},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.10854},
  archivePrefix = {arXiv},
  eprint    = {1807.10854},
  timestamp = {Mon, 13 Aug 2018 16:46:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-10854.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{darpa17ai,
  author = {Jon Launchbury},
  title = {DARPA Perspective on AI},
  year = 2017,
  url = {https://www.darpa.mil/about-us/darpa-perspective-on-ai},
  urldate = {2021-05-20}
}

@online{cc,
    author = {Common Crawl},
    title = {Common Crawl},
    year = {2021},
    url = {https://commoncrawl.org/},
    urldate = {2021-12-18}
}

@inproceedings{zhang2019ernie,
  title={{ERNIE}: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle={Proceedings of ACL 2019},
  year={2019}
}
@inproceedings{wang2018glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}
@inproceedings{peters2019knowbert,
    title = "Knowledge Enhanced Contextual Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Logan, Robert  and
      Schwartz, Roy  and
      Joshi, Vidur  and
      Singh, Sameer  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1005",
    doi = "10.18653/v1/D19-1005",
    pages = "43--54",
    abstract = "Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert{'}s runtime is comparable to BERT{'}s and it scales to large KBs.",
}

@inproceedings{logan2019barack,
    title = "{B}arack{'}s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling",
    author = "Logan, Robert  and
      Liu, Nelson F.  and
      Peters, Matthew E.  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1598",
    doi = "10.18653/v1/P19-1598",
    pages = "5962--5971",
    abstract = "Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model{'}s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.",
}
@mastersthesis{birk2020knowledge,
    title={Investigating state-of-the-art approaches to knowledge enhancing deep learning based language models},
    author={Victor Elkjær Birk},
    year={2020},
    school={Technical University of Denmark},
    address={Kongens Lyngby}
}

@article{wang2021kepler,
    author = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
    title = "{KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {176-194},
    year = {2021},
    month = {03},
    abstract = "{Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00360},
    url = {https://doi.org/10.1162/tacl\_a\_00360},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00360/1894315/tacl\_a\_00360.pdf},
}

@online{pytorchamp,
    author = {PyTorch Contributors},
    title = {Automatic Mixed Precision Package - torch.cuda.amp},
    url = {https://pytorch.org/docs/stable/amp.html},
    urldate = {2021-05-27}
}

@online{pytorchcel,
    author = {PyTorch Contributors},
    title = {CrossEntropyLoss},
    url = {https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html},
    urldate = {2021-06-09}
}

@online{huang2020amp,
    author = {Huang, Mengdi and Tekur, Chetan and Carilli, Michael},
    title = {Introducing native PyTorch automatic mixed precision for faster training on NVIDIA GPUs},
    url = {https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/},
    year = {2020},
    month = {07},
    urldate = {2021-05-28},
}

@online{mitlicense,
    author = {MIT},
    title = {The MIT License (MIT)},
    url = {https://mit-license.org/},
    urldate = {2021-05-29},
}

@online{apachelicense,
    author = {The Apache Software Foundation},
    title = {Apache License, Version 2.0},
    url = {https://www.apache.org/licenses/LICENSE-2.0.html},
    year = {2004},
    urldate = {2021-06-11},
}

@inproceedings{sohrab2018nestedner,
    author = {Mohammad Golam Sohrab and Makoto Miwa},
    title = {Deep Exhaustive Model for Nested Named Entity Recognition},
    year = {2018},
    journal = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1309.pdf},
    urldate = {2021-05-30},
    booktitle = {Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing},
    pages = {2843-2849},
}

@article{marrero2013ner,
    title = {Named Entity Recognition: Fallacies, challenges and opportunities},
    journal = {Computer Standards and Interfaces},
    volume = {35},
    number = {5},
    pages = {482-489},
    year = {2013},
    issn = {0920-5489},
    doi = {https://doi.org/10.1016/j.csi.2012.09.004},
    url = {https://www.sciencedirect.com/science/article/pii/S0920548912001080},
    author = {Mónica Marrero and Julián Urbano and Sonia Sánchez-Cuadrado and Jorge Morato and Juan Miguel Gómez-Berbís},
    keywords = {Named Entity, Named Entity Recognition, Information Extraction, Evaluation methodology, NER tools},
    abstract = {Named Entity Recognition serves as the basis for many other areas in Information Management. However, it is unclear what the meaning of Named Entity is, and yet there is a general belief that Named Entity Recognition is a solved task. In this paper we analyze the evolution of the field from a theoretical and practical point of view. We argue that the task is actually far from solved and show the consequences for the development and evaluation of tools. We discuss topics for further research with the goal of bringing the task back to the research scenario.}
}

@misc{wiki2021ner,
    author = "{Wikipedia Contributors}",
    title = "Named-entity recognition --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2021",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Named-entity_recognition&oldid=1020618016}",
    note = "[Online; accessed 1-June-2021]"
}

@misc{xiong2019wklm,
      title={Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model}, 
      author={Wenhan Xiong and Jingfei Du and William Yang Wang and Veselin Stoyanov},
      year={2019},
      eprint={1912.09637},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{corby2020kalm,
  author    = {Corby Rosset and
               Chenyan Xiong and
               Minh Phan and
               Xia Song and
               Paul N. Bennett and
               Saurabh Tiwary},
  title     = {Knowledge-Aware Language Model Pretraining},
  journal   = {CoRR},
  volume    = {abs/2007.00655},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.00655},
  archivePrefix = {arXiv},
  eprint    = {2007.00655},
  timestamp = {Mon, 06 Jul 2020 15:26:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-00655.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{wang2020kadapter,
  author    = {Ruize Wang and
               Duyu Tang and
               Nan Duan and
               Zhongyu Wei and
               Xuanjing Huang and
               Jianshu Ji and
               Guihong Cao and
               Daxin Jiang and
               Ming Zhou},
  title     = {K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters},
  journal   = {CoRR},
  volume    = {abs/2002.01808},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.01808},
  archivePrefix = {arXiv},
  eprint    = {2002.01808},
  timestamp = {Wed, 12 Feb 2020 13:04:16 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-01808.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{he2020kgplm,
  author    = {Bin He and
               Xin Jiang and
               Jinghui Xiao and
               Qun Liu},
  title     = {KgPLM: Knowledge-guided Language Model Pre-training via Generative
               and Discriminative Learning},
  journal   = {CoRR},
  volume    = {abs/2012.03551},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.03551},
  archivePrefix = {arXiv},
  eprint    = {2012.03551},
  timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-03551.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{ruder21ner,
  author = {Sebastian Ruder},
  title = {NLP-progress: Named Enitty Recognition},
  year = 2021,
  url = {http://nlpprogress.com/english/named_entity_recognition.html},
  urldate = {2021-06-02}
}

@online{pwc21ner,
  author = {Papers With Code Contributors},
  title = {Papers With Code: Named Entity Recognition},
  year = 2021,
  url = {https://paperswithcode.com/task/named-entity-recognition-ner},
  urldate = {2021-06-02}
}

@inproceedings{enevoldsen2020dacy,
    title={DaCy: A SpaCy NLP Pipeline for Danish},
    author={Enevoldsen, Kenneth},
    year={2021}
}

@inproceedings{howardruder2018universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@online{klein2011amdahl,
    title = {Amdahl's Law},
    author = {Joel F. Klein},
    month = {mar},
    year = {2011},
    url = {https://demonstrations.wolfram.com/AmdahlsLaw/},
    urldate = {2021-06-11},
}

@inproceedings{kirkedal2019lacunae,
    title = "The Lacunae of {D}anish Natural Language Processing",
    author = "Kirkedal, Andreas  and
      Plank, Barbara  and
      Derczynski, Leon  and
      Schluter, Natalie",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://www.aclweb.org/anthology/W19-6141",
    pages = "356--362",
    abstract = "Danish is a North Germanic language spoken principally in Denmark, a country with a long tradition of technological and scientific innovation. However, the language has received relatively little attention from a technological perspective. In this paper, we review Natural Language Processing (NLP) research, digital resources and tools which have been developed for Danish. We find that availability of models and tools is limited, which calls for work that lifts Danish NLP a step closer to the privileged languages. Dansk abstrakt: Dansk er et nordgermansk sprog, talt prim{\ae}rt i kongeriget Danmark, et land med st{\ae}rk tradition for teknologisk og videnskabelig innovation. Det danske sprog har imidlertid v{\ae}ret genstand for relativt begr{\ae}nset opm{\ae}rksomhed, teknologisk set. I denne artikel gennemg{\aa}r vi sprogteknologi-forskning, -ressourcer og -v{\ae}rkt{\o}jer udviklet for dansk. Vi konkluderer at der eksisterer et f{\aa}tal af modeller og v{\ae}rkt{\o}jer, hvilket indbyder til forskning som l{\o}fter dansk sprogteknologi i niveau med mere priviligerede sprog.",
}

@inproceedings{danlp2021,
    title = {{DaNLP}: An open-source toolkit for Danish Natural Language Processing},
    author = {Brogaard Pauli, Amalie  and
      Barrett, Maria  and
      Lacroix, Ophélie  and
      Hvingelby, Rasmus},
    booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021)},
    month = june,
    year = "2021"
}

@online{arup21awesome,
  author = {Finn Årup Nielsen},
  title = {Awesome Danish: A curated list of awesome resources for Danish language technology},
  year = 2021,
  url = {https://github.com/fnielsen/awesome-danish/blob/master/README.md},
  urldate = {2021-06-11}
}

@inproceedings{grave2018learning,
    title = "Learning Word Vectors for 157 Languages",
    author = "Grave, Edouard  and
      Bojanowski, Piotr  and
      Gupta, Prakhar  and
      Joulin, Armand  and
      Mikolov, Tomas",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1550",
}

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{kjeldgaard2020nerda,
  title = {NERDA},
  author = {Kjeldgaard, Lars and Nielsen, Lukas},
  year = {2020},
  publisher = {{GitHub}},
  url = {https://github.com/ebanalyse/NERDA}
}

@online{pytorchrep,
    author = {PyTorch Contributors},
    title = {Reproducability},
    url = {https://pytorch.org/docs/stable/notes/randomness.html},
    urldate = {2021-06-09}
}

@book{bishop2006pattern,
    author = {Bishop, Christopher M.},
    title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
    year = {2006},
    isbn = {0387310738},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg}
}

@misc{mcinnes2020umap,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{maaten2008tsne,
  added-at = {2015-06-19T12:07:15.000+0200},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  biburl = {https://www.bibsonomy.org/bibtex/28b9aebb404ad4a4c6a436ea413550b30/lopusz_kdd},
  interhash = {370ba8b9e1909b61880a6f47c93bcd49},
  intrahash = {8b9aebb404ad4a4c6a436ea413550b30},
  journal = {Journal of Machine Learning Research},
  keywords = {dimensionality_reduction tSNE visualization},
  pages = {2579--2605},
  timestamp = {2015-08-19T15:19:11.000+0200},
  title = {Visualizing Data using {t-SNE} },
  url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
  volume = 9,
  year = 2008
}
@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
} 

@article{mcinnes2018umap-software,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}

@inproceedings{krogh1991weight,
    author = {Krogh, Anders and Hertz, John A.},
    title = {A Simple Weight Decay Can Improve Generalization},
    year = {1991},
    isbn = {1558602224},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA},
    abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
    booktitle = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
    pages = {950--957},
    numpages = {8},
    location = {Denver, Colorado},
    series = {NIPS'91}
}

@misc{hutter2019adamw,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lecun2015deep,
    author = {LeCun, Yann and Bengio, Y. and Hinton, Geoffrey},
    year = {2015},
    month = {05},
    pages = {436-44},
    title = {Deep Learning},
    volume = {521},
    journal = {Nature},
    doi = {10.1038/nature14539}
}
@INPROCEEDINGS{sun2017data,
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}, 
  year={2017},
  volume={},
  number={},
  pages={843-852},
  doi={10.1109/ICCV.2017.97}
}

@misc{seqeval,
  title={{seqeval}: A Python framework for sequence labeling evaluation},
  url={https://github.com/chakki-works/seqeval},
  note={Software available from https://github.com/chakki-works/seqeval},
  author={Hiroki Nakayama},
  year={2018},
}

@article{batista2018eval,
 author  = {Batista, David S.},
 date    = {2018-05-09},
 title   = {Named-Entity evaluation metrics based on entity-level},
 journal = {David S. Batista Blog},
 url     = {http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/},
 urldate = {2021-06-19}
}

@inproceedings{hochreiter1997lstm,
    author = {Sepp Hochreiter and Jürgen Schmidhuber},
    title = {Long Short-Term Memory},
    booktitle = {Neural Computation},
    year = {1997},
    publisher = {MIT Press},
    pages = {1735--1780},
    numpages = {46},
    url = {https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory},
    urldate = {2021-06-18},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{schuster1997birnn,
    author={Schuster, M. and Paliwal, K.K.},
    journal={IEEE Transactions on Signal Processing},
    title={Bidirectional recurrent neural networks},
    year={1997},
    volume={45},
    number={11},
    pages={2673-2681},
    doi={10.1109/78.650093},
}
@misc{conneau2020unsupervised,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@inproceedings{Radford2019gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@article{wu2016tokenize,
    title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
    year	= {2016},
    URL	= {http://arxiv.org/abs/1609.08144},
    journal	= {CoRR},
    volume	= {abs/1609.08144}
}

@article{rashford2018gpt,
    title = {Improving language understanding with unsupervised learning},
    author = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
    year = {2018},
    url = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
    urldate = {2021-06-24},
}

@inproceedings{isem2013daiber,
  title = {Improving Efficiency and Accuracy in Multilingual Entity Extraction},
  author = {Joachim Daiber and Max Jakob and Chris Hokamp and Pablo N. Mendes},
  year = {2013},
  booktitle = {Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)}
}

@article{brochier2021wikilink,
  author    = {Robin Brochier and
               Fr{\'{e}}d{\'{e}}ric B{\'{e}}chet},
  title     = {Predicting Links on Wikipedia with Anchor Text Information},
  journal   = {CoRR},
  volume    = {abs/2105.11734},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.11734},
  archivePrefix = {arXiv},
  eprint    = {2105.11734},
  timestamp = {Tue, 01 Jun 2021 18:07:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-11734.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hansen2017esa,
title = "Open semantic analysis: The case of word level semantics in Danish",
abstract = "The present research is motivated by the need for accessible and efficient tools for automated semantic analysis in Danish. We are interested in tools that are completely open, so they can be used by a critical public, in public administration, non-governmental organizations and businesses. We describe data-driven models for Danish semantic relatedness, word intrusion and sentiment prediction. Open Danish corpora were assembled and unsupervised learning implemented for explicit semantic analysis and with Gensim{\textquoteright}s Word2vec model. We evaluate the performance of the two models on three different annotated word datasets. We test the semantic representations{\textquoteright} alignment with single word sentiment using supervised learning. We find that logistic regression and large random forests perform well with Word2vec features.",
author = "Nielsen, {Finn {\AA}rup} and Hansen, {Lars Kai}",
year = "2017",
language = "English",
booktitle = "Proceedings of 8th Language and Technology Conference",
note = "8th Language and Technology Conference , LTC 2017 ; Conference date: 17-11-2017 Through 19-11-2017",
}

@inproceedings{derc2021giga,
 title = {{The Danish Gigaword Corpus}},
 author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn Årup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystrøm and Daniel Varab},
 year = 2021,
 booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},
 publisher = {NEALT}
}

@phdthesis{nielsen2020textsum,
  author  = {Lukas Christian Nielsen and Sebastian Lindegaard Veile},
  title   = {Automatic Text Summarization For Danish Using BERT},
  school  = {IT University of Copenhagen},
  year    = {2020},
  type    = {Master's Thesis}
}

@online{kromann2004cdt,
  author = {M.T. Kromann and S.K. Lynge},
  title = {Copenhagen Dependency Treebank},
  year = 2004,
  school = {Department of Computational Linguistics, Copenhagen Business School},
  url = {https://github.com/mbkromann/copenhagen-dependency-treebank},
  urldate = {2021-06-24}
}

@online{raj2019bert,
  author = {Bharat S Raj},
  title = {Understanding BERT: Is it a Game Changer in NLP?},
  year = 2019,
  url = {https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad},
  urldate = {2021-06-24}
}

@misc{hendrycks2020gelu,
      title={Gaussian Error Linear Units (GELUs)},
      author={Dan Hendrycks and Kevin Gimpel},
      year={2020},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{GONG2020131,
    title = {Preserving differential privacy in deep neural networks with relevance-based adaptive noise imposition},
    journal = {Neural Networks},
    volume = {125},
    pages = {131-141},
    year = {2020},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/j.neunet.2020.02.001},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608020300460},
    author = {Maoguo Gong and Ke Pan and Yu Xie and A.K. Qin and Zedong Tang},
    keywords = {Deep neural networks, Differential privacy, Relevance analysis},
    abstract = {In recent years, deep learning achieves remarkable results in the field of artificial intelligence. However, the training process of deep neural networks may cause the leakage of individual privacy. Given the model and some background information of the target individual, the adversary can maliciously infer the sensitive feature of the target individual. Therefore, it is imperative to preserve the sensitive information in the training data. Differential privacy is a state-of-the-art paradigm for providing the privacy guarantee of datasets, which protects the private and sensitive information from the attack of adversaries significantly. However, the existing privacy-preserving models based on differential privacy are less than satisfactory since traditional approaches always inject the same amount of noise into parameters to preserve the sensitive information, which may impact the trade-off between the model utility and the privacy guarantee of training data. In this paper, we present a general differentially private deep neural networks learning framework based on relevance analysis, which aims to bridge the gap between private and non-private models while providing an effective privacy guarantee of sensitive information. The proposed model perturbs gradients according to the relevance between neurons in different layers and the model output. Specifically, during the process of backward propagation, more noise is added to gradients of neurons that have less relevance to the model output, and vice-versa. Experiments on five real datasets demonstrate that our mechanism not only bridges the gap between private and non-private models, but also prevents the disclosure of sensitive information effectively.}
}

@inproceedings{sigurbergsson-derczynski-2020-offensive,
    title = "Offensive Language and Hate Speech Detection for {D}anish",
    author = "Sigurbergsson, Gudbjartur Ingi  and
      Derczynski, Leon",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.430",
    pages = "3498--3508",
    abstract = "The presence of offensive language on social media platforms and the implications this poses is becoming a major concern in modern society. Given the enormous amount of content created every day, automatic methods are required to detect and deal with this type of content. Until now, most of the research has focused on solving the problem for the English language, while the problem is multilingual. We construct a Danish dataset DKhate containing user-generated comments from various social media platforms, and to our knowledge, the first of its kind, annotated for various types and target of offensive language. We develop four automatic classification systems, each designed to work for both the English and the Danish language. In the detection of offensive language in English, the best performing system achieves a macro averaged F1-score of 0.74, and the best performing system for Danish achieves a macro averaged F1-score of 0.70. In the detection of whether or not an offensive post is targeted, the best performing system for English achieves a macro averaged F1-score of 0.62, while the best performing system for Danish achieves a macro averaged F1-score of 0.73. Finally, in the detection of the target type in a targeted offensive post, the best performing system for English achieves a macro averaged F1-score of 0.56, and the best performing system for Danish achieves a macro averaged F1-score of 0.63. Our work for both the English and the Danish language captures the type and targets of offensive language, and present automatic methods for detecting different kinds of offensive language such as hate speech and cyberbullying.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{taylor2020alignment,
  title={Alignment for Advanced Machine Learning Systems},
  author={Jessica Taylor and Eliezer Yudkowsky and Patrick LaVictoire and Andrew Critch},
  year={2020}
}

@article{brockhoff2018stat,
    title={Introduction to Statistics at DTU},
    author={Per B. Brockhoff and Jan K. Møller and Elisabeth W. Andersen and Peder Bacher and Lasse E. Christiansen},
    url= {https://02323.compute.dtu.dk/enotes/book-IntroStatistics},
    journal={},
    year={2018},
}

@inproceedings{chen2021kb-vlp,
author = {Chen, Kezhen and Huang, Qiuyuan and Bisk, Yonatan and McDuff, Daniel and Gao, Jianfeng},
title = {KB-VLP: Knowledge Based Vision and Language Pretraining},
booktitle = {Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. ICML, workshop, 2021},
year = {2021},
month = {July},
abstract = {Transformer-based pretraining techniques have achieved impressive performance on learning
cross-model representations for various multimodal tasks. However, off-the-shelf models do
not take advantage of commonsense knowledge and logical reasoning that are crucial to many real world tasks. To this end, we introduce a novel pretraining approach - Knowledge Based Vision and
Language Pretraining (KB-VLP) - which uses knowledge graph embeddings extracted from text
and detected image object tags to enhance the learning of semantically aligned and knowledge aware
representations, and improve the models generalization, and interpretability. KB-VLP is pretrained on a large image-text corpus and automatically extracted knowledge embeddings, and then finetuned on several downstream vision language tasks. Experiments show that KB-VLP significantly improves the performance on VQA, GQA, NLVR2 and OKVQA tasks compared with the baselines.},
url = {https://www.microsoft.com/en-us/research/publication/kb-vlp-knowledge-based-vision-and-language-pretraining-2/},
}
@inproceedings{sun2020colake,
    title = "{C}o{LAKE}: Contextualized Language and Knowledge Embedding",
    author = "Sun, Tianxiang  and
      Shao, Yunfan  and
      Qiu, Xipeng  and
      Guo, Qipeng  and
      Hu, Yaru  and
      Huang, Xuanjing  and
      Zhang, Zheng",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.327",
    doi = "10.18653/v1/2020.coling-main.327",
    pages = "3660--3670",
    abstract = "With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing models consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these models. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the knowledge context of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and language context, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.",
}

@inproceedings{boye2021ing,
    title = "Data Scientists Across Fields Spent Their Holidays Producing a new Danish Language Model (translated)",
    author = "Magnus Boye",
    publisher = "ING/Datatech",
    month = aug,
    year = "2021",
    url = "https://pro.ing.dk/datatech/artikel/data-scientists-paa-tvaers-af-brancher-brugte-ferie-paa-lave-ny-dansk-sprogmodel",
}

@inproceedings{nielsen2021scandi,
    title = "ScandiNER - Named Entity Recognition model for Scandinavian Languages",
    author = "Dan Saattrup Nielsen",
    publisher = "Hugging Face Repositories",
    month = sep,
    year = "2021",
    url = "https://huggingface.co/saattrupdan/nbailab-base-ner-scandi",
}
