\documentclass[10pt, a4paper]{article}
\usepackage{lrec2022} % this is the new LREC2022 Style
\usepackage{multibib}
\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{tikz}
\usepackage{}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{float}
\usepackage{changepage}
\usepackage[nottoc, numbib]{tocbibind}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage[bottom]{footmisc}
\usepackage{MediocreMike}

% for eps graphics
%%% References and Labels
%%% Reference labels without a punctuation 
% courtesy of Marc Schulder , uni Hamburg ****************
\usepackage{titlesec}
%\titleformat{\section}{\normalfont\large\bf\center}{\thesection.}{1em}{}
\titleformat{\section}{\normalfont\large\bfseries\center}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\SmallTitleFont\bfseries\raggedright}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries\raggedright}{\thesubsubsection.}{1em}{}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}
%  ed 
\newcommand{\softmax}{\operatorname{softmax}}

\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xstring}

\usepackage{color}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}

\title{DaLUKE: Strengthening Danish NLP Using Weak Knowledge-Enhancement}

\name{Søren Winkel Holm, Asger Laurits Schultz, ...} 

\address{Technical University of Denmark, 2800 Kgs. Lyngby\\
         \{s183911, s183912, ...\}@dtu.dk\\}


\abstract{
    t
\newline \Keywords{NLP, Transformers, Entity-based Learning}
}

\begin{document}

\maketitleabstract

\section{Introduction}
The use of emerging languages technologies in industry has grown with positive impact in bussines and society, but this impact is largest in high-resource languages such as English motivating a focus on developing diverse methods for languages with much less available textual data \cite{hedderich2021survey}.
Languages such as Danish, with around 6 million speakers, fall in a size class where NLP ressources and an NLP tradition exist but with data of so limited magnitude that models such as large-scale transformers underperform.
In such domains, recent advances in combining explicit modeling of language knowledge with flexible SOTA models can be the key to get closer to the glory of the English-speaking models.
DaLUKE is such an attempt, using the general model idea of LUKE \cite{yamada2020luke} and adding custom solutions for the case of Danish to measure the potential and applicability of this combination.


\subsection{Related Models}
\paragraph{Knowledge-based Pretraining Augmentations}
\begin{itemize}
    \item KEPLER
    \item WKLM
    \item KALM
    \item K-Adapter
    \item KGpLM
    \item KB-VLP
    \item CoLAKE
\end{itemize}
\paragraph{Danish NLP}
\begin{itemize}
    \item DaBERT
    \item DaCy
\end{itemize}


\section{Methods}

\subsection{Pretraining}
Six pretrained models are produced using various combinations of three variables.
The experiments are listed in table \ref{tab:pretraining-experiments}.
\begin{table}[H]
%    TODO Make table fit within column
    \centering
    \footnotesize
    \begin{tabular}{l|ccc}
        Experiment      &RoBERTa &Auto-annotated&Reduced entity	\\
        name&version&entities&dimensionality\\\hline
        Baseline        &Base	& &	\\
        Base+auto       &Base	&$\checkmark$ &	\\
        Base+ent        &Base	& &$\checkmark$	\\
        Base+auto+ent   &Base	&$\checkmark$ &$\checkmark$	\\
        Large           &Large	& \\
        Large+auto      &Large   &$\checkmark$   &
    \end{tabular}
    \caption{Pretraining experiments.
    Hyperparameters were kept constant across all and are listed in table \ref{tab:pretraining-hyperparameters}.}
    \label{tab:pretraining-experiments}
\end{table}\noindent
\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabular}{l|r}
        Parameter                           &Value\\\hline
        Batches                             &3,000\\
        Batch size                          &4,080\\
        Peak learning rate                  &$ 10^{-4} $\\
        Learning rate warmup proportion     &6\pro\\
        Entity embedding size               &256\\
        AdamW weight decay                  &0.01\\
        Word masking probability            &15\pro\\
        Word unmasking probability          &10\pro\\
        Random word replacement probability &10\pro\\
        Entity masking probability          &15\pro
    \end{tabular}
    \caption{Hyperparameters for pretraining experiments.}
    \label{tab:pretraining-hyperparameters}
\end{table}\noindent

\paragraph{Dataset}
The pretraining comes from two sources: The Danish Wikipedia and the Danish Gigaword Corpus (DAGW) \cite{derc2021giga}.
DaLUKE follows LUKE's pretraining task, which is an extension of BERT's masked language modelling \cite{devlin2019bert} \cite{yamada2020luke}.
For this, both a large text corpus and entity annotations are needed.
While the Danish Wikipedia is rather limited, it does contain entity annotations unlike DAGW.
Part of Wikipedia is included in DAGW, but without annotations, so this part of DAGW is replaced by the Danish Wikipedia with annotations (downloaded Sep. 21st 2021).
Special articles were removed from Wikipedia -- most notably category and image pages.

Furthermore, DAGW also contains a subset of the Common Crawl \cite{cc}.
This was found to be of relatively low quality and so was removed.

\paragraph{Auto-annotated entities}
As with most low-resource languages, the Danish Wikipedia fairly small, and it is the only source of entity labels in the dataset.
Even though the full dataset contains more than 900 million words, the entities number only 4.7 million, or about one entity per 200 words.
Auto-annotation is an attempt to remedy this.
Using the entity vocabulary from Wikipedia, at every word start in the dataset it is checked if the next $ n $ characters match an entity (ignoring casing).
If so, all words covered, whole or partially, by the characters, and the cursor jumps to the next word start, where the process is repeated.
To work, all $ n $ in an interval need to be checked.
The lower bound is set to 5 to prevent too many wrong matches and the upper bound to 48 to limit runtime.
While this algorithm is rather crude, it does increase the number of entities to 115 million.
When inspecting some of the annotations, very few incorrect ones were found.
An drawback of this method is that the characters need to match one-to-one, so there will be entities that are not annotated.

\paragraph{Reduced entity dimensionality}
LUKE models words and entities in the same $ L $ dimensional latent space \cite{yamada2020luke}.
However, due to the lower number of entities and the simpler structure compared to words, we propose modelling them in a lower, $ l $ dimensional latent space.
This requires adding some additional matrices in the attention blocks compared to LUKE and modifying others, causing the total number of parameters to drop (assuming that $ L $ is sufficiently larger than $ l $).
Tabel \ref{tab:mats} shows all parameter matrices in LUKE and DaLUKE.
\begin{table}[H]
    \centering
    \footnotesize
%    TODO Maybe move to appendix?
    \begin{tabular}{l|c|c}
        Matrix&Dimension in LUKE&Dimension in DaLUKE\\\hline
        $ \mathbf Q $&$ L\times L $&$ L\times L $\\
        $ \mathbf Q_{w2e} $&$ L\times L $&$ L\times l $\\
        $ \mathbf Q_{e2w} $&$ L\times L $&$ l\times L $\\
        $ \mathbf Q_{e2e} $&$ L\times L $&$ l\times l $\\
        $ \mathbf K $&$ L\times L $&$ L\times L $\\
        $ \mathbf K_e $&&$ l\times l $\\
        $ \mathbf V $&$ L\times L $&$ L\times L $\\
        $ \mathbf V_{w2e} $&&$ L\times l $\\
        $ \mathbf V_{e2w} $&&$ l\times L $\\
        $ \mathbf V_{e2e} $&&$ l\times l $\\
    \end{tabular}
    \caption{Comparison of the matrix dimensions in the transformer blocks of LUKE and DaLUKE.}
    \label{tab:mats}
\end{table}\noindent
Formally, a sequency of $ m $ word token representations of dimension $ L $ and $ n $ entity token representations of dimension $ l $ is structured as two matrices:
\begin{align*}
    &\mathbf W = \begin{bmatrix}
        |&&|\\
        \mathbf x_1&\cdots&\mathbf x_m\\
        |&&|
    \end{bmatrix} \in \RR^{L\times m}\\
    &\mathbf E = \begin{bmatrix}
        |&&|\\
        \mathbf x_{m+1}&\cdots&\mathbf x_{m+n}\\
        |&&|
    \end{bmatrix} \in \RR^{l\times n}
\end{align*}
The attention between the tokens are structured into four matrices, with softmax being applied row-wise:
\begin{align*}
        &\mathbf A = \softmax\left(\frac{\transpose{\mathbf W}\transpose{\mathbf K}\mathbf Q\mathbf W}{\sqrt{L}}\right)\in\RR^{m\times m}\\
        &\mathbf A_{w2e} = \softmax\left(\frac{\transpose{\mathbf E}\transpose{\mathbf K}\mathbf Q_{w2e}\mathbf W}{\sqrt{l}}\right)\in\RR^{n\times m}\\
        &\mathbf A_{e2w} = \softmax\left(\frac{\transpose{\mathbf W}\transpose{\mathbf K_e}\mathbf Q_{e2w}\mathbf E}{\sqrt{L}}\right)\in\RR^{m\times n}\\
        &\mathbf A_{e2e} = \softmax\left(\frac{\transpose{\mathbf E}\transpose{\mathbf K_e}\mathbf Q_{e2e}\mathbf E}{\sqrt{l}}\right)\in\RR^{n\times n}
\end{align*}
Finally, the contextualized representations, $ \mathbf W_{c}\in\RR^{L\times m} $ and $ \mathbf E_{c}\in\RR^{l\times n} $, are calculated as
\begin{align*}
    &\transpose{\mathbf W_c} = \mathbf A\transpose{\mathbf W}\mathbf V
        + \mathbf A_{e2w}\transpose{\mathbf E}\mathbf V_{e2w} \\
        &\transpose{\mathbf E_c} = \mathbf A_{e2e}\transpose{\mathbf E}\mathbf V_{e2e}
        + \mathbf A_{w2e}\transpose{\mathbf W}\mathbf V_{w2e}
\end{align*}
Notice that while the calculations in the attention blocks here are expressed as purely linear, e.g. $ \mathbf A\mathbf x $, all of the matrices also have a bias term, e.g. $ \mathbf A\mathbf x+\mathbf b $.
These biases do not change any of the dimensionality considerations and have thus been left out for simplicity.

\section{Results}

\section{Discussion}
%Self-attention går i stykker pga. nødvendig transponering

\section{Bibliographical References}\label{reference}

\bibliographystyle{lrec2022-bib}
\bibliography{references}

\section{Language Resource References}
\label{lr:ref}
\bibliographystylelanguageresource{lrec2022-bib}
\bibliographylanguageresource{languageresource}

\end{document}
