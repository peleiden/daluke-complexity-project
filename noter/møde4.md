# Vores input

- DaLUKE er på DaNLP
- Bygning af DagW virker!
- X-RoBERTa virker i træningsloop, men betatests viser bekymrende 0-accuracy (tal om padding)
- Dårlig kvalitet på dele af dagw
- Validering virker (1/100-del)
- Regner med træning indenfor en uge

# Noter

- Gennemgang af RoBERTa-implementering og validering
- Gennemgang af DAGW-status og -kvalitet
- Diskussion af fordele/ulempe ved autoannotering
- Overvej om brug DaLUKE trænet på ren Wikipedia til autoannotering
- Vent med at starte Google, til alle eksperimenter er klar, og al koden helt sikkert virker
    - Forhåbentligt om to eller tre uger
- Case sensitive forventes at give en stor forbedring til NER
- Diskussion af RoBERTa-udfordringer
- Sendt eksempler på fejldokumenter til Finn Årup + send fuld liste bagefter
