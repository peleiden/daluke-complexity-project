% !TeX spellcheck = en_GB
% !BIB TS-program = biber
\documentclass[12pt, fleqn]{article}

\input{preamble.tex}

\addbibresource{references.bib}

\chead{}
\rhead{Technical University of Denmark}
\rfoot{Page \thepage{}~of \pageref{LastPage}}

\graphicspath{{imgs/}{../imgs/}}

\title{DaLUKE: Strengthening Danish NLP Using Weak Knowledge-Enhancement}
\author{Søren Winkel Holm, Asger Laurits Schulz}
\date{\today}
\linespread{1.15}

\begin{document}
% Avoid warnings
\setlength{\headheight}{15pt}
\addtolength{\topmargin}{-2.5pt}
\maketitle

\begin{abstract}
    \begin{itemize}
        \item Fokusér på problemet med NLP i middelressourcesprog generelt
        \item Dansk er et case study, hvor vi undersøger LUKE-arkitekturen og vidensenhancement generelt som mulig løsning
        \item Inkludér reklame for kode
    \end{itemize}
\end{abstract}
\section{Introduction}%
\label{sec:Introduction}
\begin{itemize}
    \item Kort gennemgang af status i dansk NLP
    \item Kort introduktion af LUKE + nævn andre vidensenhancement-metoder
    \item Det mest generelle, motiverende spørgsmål: Hvad skal der til for at få det elegant LUKE-ideal til at virke med meget mere begrænset data?
    \item Mindre ressourcestærkt sprog => mindre ressourcestærke anvendere, så fokus på lille model
\end{itemize}

\section{Methods}%
\label{sec:Methods}

\begin{itemize}
    \item Data: Augmentering og forskellige kilder
    \item Hovedmodel: Hvad var forskellig fra DaLUKE
    \item Fremgangsmåde for træning
    \item Lille model: Fremgangsmåde for destillering/pruning
    \item Præsentér vores nye arkitektur til lavere entitetsdimension
    \item Præsentér NER-opgaven
\end{itemize}


\begin{table}[H]
    \centering
    \footnotesize
        \begin{tabular}{l|ll}
            Name                & Data & Base Model\\
            \hline
            Control             & Da. Wiki. w. entity links, Da. Gigaword & RoBERTa Base \\
            Auto-annotated      & Da. Wiki. w. entity links, Da. Gigaword with auto. entities & RoBERTa Base\\
            Big model           & Da. Wiki. w. entity links, Da. Gigaword & RoBERTa Large\\
            Low entity dim.     & Da. Wiki. w. entity links, Da. Gigaword & 252-dimensional ent., RoBERTa Base 
        \end{tabular}
    \caption{
        Learning rate: \(1.2\ctp{-4}\), batch size: 8160 examples
    }
    \label{tab:experiments}
\end{table}\noindent

\section{Results}%
\label{sec:Results}
\begin{itemize}
    \item Prætræningsperformance med forskellige modeller
    \item NER-performance sammenlignet med dansk niveau
\end{itemize}

\section{Discussion}%
\label{sec:Discussion}
\begin{itemize}
    \item Prætrænings-eksperimenter: Data, arkitektur og transferlæring kontrolleret
    \item En smule undersøgelse af maskeret opgave/downstream fejl
    \item Konklusioner: AI-succes bliver bestemt af nogle andre underliggende parametre i højressourcedomænet end i den dataknappe situation
\end{itemize}

% Smaller bib text
\renewcommand*{\bibfont}{\normalfont\footnotesize}
\printbibliography[heading=bibintoc]

\appendix

%\begin{tabular}[t]{l>{}l>{}l>{}l>{}l>{}l>{}l>{}l}
%\multicolumn{1}{c}{ } & \multicolumn{7}{c}{Stochastic Augmentations} \\
%\multicolumn{1}{c}{ } & \multicolumn{4}{c}{Names} & \multicolumn{3}{c}{Keystroke Errors} \\
%Model & Danish & Muslim & Female & Male & 2\% & 5\% & 15\%\\
%DaLUKE & \textbf{84.3 (0.6)} & \textbf{82.1 (0.9)*} & \textbf{85.2 (0.6)*} & \textbf{84.3 (0.8)} & \textbf{71.1 (1.6)*} & \textbf{57.3 (1.7)*} & \textbf{33.0 (1.7)*}\\
%\end{tabular}


\end{document}

